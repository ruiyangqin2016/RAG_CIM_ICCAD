{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import losses\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import os, random\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "class CustomTripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0, distance_metric='euclidean', positive_margin=0.5):\n",
    "        super(CustomTripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.positive_margin = positive_margin\n",
    "        self.distance_metric = distance_metric\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            distance_pos = torch.sqrt(torch.sum((anchor - positive) ** 2, dim=1))\n",
    "            distance_neg = torch.sqrt(torch.sum((anchor - negative) ** 2, dim=1))\n",
    "        elif self.distance_metric == 'cosine':\n",
    "            distance_pos = 1 - F.cosine_similarity(anchor, positive)\n",
    "            distance_neg = 1 - F.cosine_similarity(anchor, negative)\n",
    "\n",
    "        # Triplet loss component\n",
    "        triplet_loss = F.relu(distance_pos - distance_neg + self.margin)\n",
    "        \n",
    "        # Positive distance component\n",
    "        # This part aims to increase the distance between positive samples up to a certain margin (positive_margin)\n",
    "        positive_distance_loss = F.relu(self.positive_margin - distance_pos)\n",
    "\n",
    "        # Combine both components\n",
    "        # Note: You might need to adjust the weight of positive_distance_loss to balance both objectives\n",
    "        loss = triplet_loss + positive_distance_loss\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "class SentenceEmbeddingModel(nn.Module):\n",
    "    def __init__(self, model_id, dropout_rate=0.1, noise_std=0.05):\n",
    "        super().__init__()\n",
    "        self.sentence_transformer = SentenceTransformer(model_id)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(384, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 384)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.noise_std = noise_std  \n",
    "    \n",
    "    def forward(self, sentences, apply_dropout=False, apply_noise=False):\n",
    "        embeddings = self.sentence_transformer.encode(sentences, convert_to_tensor=True, batch_size=len(sentences))\n",
    "        if apply_dropout:\n",
    "            embeddings = self.dropout(embeddings)\n",
    "        if apply_noise:\n",
    "            noise = torch.randn_like(embeddings) * self.noise_std\n",
    "            embeddings = embeddings + noise\n",
    "        outputs = self.mlp(embeddings)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def encode(self, sentences, convert_to_tensor=True):\n",
    "        return self.sentence_transformer.encode(sentences, convert_to_tensor=convert_to_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process user: 7005607\n",
      "Process user: 7004451\n",
      "Process user: 7005507\n",
      "Constructing anchor data...\n",
      "Constructing positive data...\n",
      "Constructing negative data...\n",
      "Constructing training examples...\n"
     ]
    }
   ],
   "source": [
    "data_path = './Data/LaMP-1/tiny_data.json'\n",
    "data = json.load(open(data_path, 'r'))\n",
    "user_wise_RAG_dict = {}\n",
    "raw_data = []\n",
    "for point in data:\n",
    "    print(f'Process user: {point[\"user_id\"]}')\n",
    "    if point['user_id'] not in user_wise_RAG_dict:\n",
    "        user_wise_RAG_dict[point['user_id']] = []\n",
    "    for q in point['profile']:\n",
    "        title = q['title']\n",
    "        abstract = q['abstract']\n",
    "        user_history = f'abstract: {abstract} title: {title}'\n",
    "        user_wise_RAG_dict[point['user_id']].append(user_history)\n",
    "        temp_dict = {\n",
    "            'user_id': point['user_id'],\n",
    "            'title': title,\n",
    "            'abstract': abstract\n",
    "        }\n",
    "        raw_data.append(temp_dict)\n",
    "\n",
    "train_examples = []\n",
    "\n",
    "### Construct the anchor data ### \n",
    "anchor_data = []\n",
    "print('Constructing anchor data...')\n",
    "for dp in raw_data:\n",
    "    temp_dp = f'title: {dp[\"title\"]} \\n abstract: {dp[\"abstract\"]}'\n",
    "    anchor_data.append(temp_dp)\n",
    "\n",
    "### Construct positive data ###\n",
    "positive_data_dict = {}\n",
    "print('Constructing positive data...')\n",
    "for anchor_sentence in anchor_data:\n",
    "    positive_data_dict[anchor_sentence] = []\n",
    "    duplicated_sentences = [anchor_sentence] * 5\n",
    "    positive_data_dict[anchor_sentence] = duplicated_sentences\n",
    "\n",
    "### Construct negative data ###\n",
    "print('Constructing negative data...')\n",
    "negative_data_dict = {}\n",
    "for dp, anchor_sentence in zip(raw_data, anchor_data):\n",
    "    negative_data_dict[anchor_sentence] = []\n",
    "    # Negative data is the current abstract concatenated with other titles\n",
    "    current_abstract = dp['abstract']\n",
    "    other_titles = []\n",
    "    for dp_ in raw_data:\n",
    "        if dp_['abstract'] != current_abstract:\n",
    "            other_titles.append(dp_['title'])\n",
    "    # Randomly sample 5 titles\n",
    "    other_titles = random.sample(other_titles, 5)\n",
    "    for title in other_titles:\n",
    "        neg_sentence = f'title: {title} \\n abstract: {current_abstract}'\n",
    "        negative_data_dict[anchor_sentence].append(neg_sentence)\n",
    "        \n",
    "print('Constructing training examples...')\n",
    "for anchor_sentence in anchor_data:\n",
    "    for i in range(5):\n",
    "        pos_sentence = positive_data_dict[anchor_sentence][i]\n",
    "        neg_sentence = negative_data_dict[anchor_sentence][i]\n",
    "        train_examples.append((anchor_sentence, pos_sentence, neg_sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def evaluate_sts(model, sts_dataset):\n",
    "    cos_sim = nn.CosineSimilarity()\n",
    "    predicted_scores = []\n",
    "    actual_scores = []\n",
    "\n",
    "    eval_split = 'dev'\n",
    "    sentences1 = sts_dataset[eval_split]['sentence1']\n",
    "    sentences2 = sts_dataset[eval_split]['sentence2']\n",
    "    actual_scores = sts_dataset[eval_split]['similarity_score']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(sentences1)):\n",
    "            sentence1, sentence2 = sentences1[idx], sentences2[idx]\n",
    "            # actual_score = actual_scores[idx]\n",
    "            embedding1 = model.encode([sentence1], convert_to_tensor=True)\n",
    "            embedding2 = model.encode([sentence2], convert_to_tensor=True)\n",
    "            predicted_score = cos_sim(embedding1, embedding2).cpu().numpy()\n",
    "            predicted_scores.append(predicted_score)\n",
    "\n",
    "    spearman_corr = spearmanr(predicted_scores, actual_scores)[0]\n",
    "    return spearman_corr\n",
    "\n",
    "def calculate_distances(model, dataloader, distance_metric='euclidean'):\n",
    "    cos_sim = nn.CosineSimilarity()\n",
    "    positive_distances = []\n",
    "    negative_distances = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            anchor_sentences, positive_sentences, negative_sentences = batch\n",
    "            sentences = anchor_sentences + negative_sentences\n",
    "            embeddings = model(sentences)\n",
    "            anchor_embeddings = embeddings[:len(anchor_sentences)]\n",
    "            negative_embeddings = embeddings[len(anchor_sentences):]\n",
    "            positive_embeddings = model(positive_sentences, apply_dropout=True)\n",
    "\n",
    "            # 计算正样本与锚点之间的距离\n",
    "            if distance_metric == 'euclidean':\n",
    "                pos_distance = torch.sqrt(torch.sum((anchor_embeddings - positive_embeddings) ** 2, dim=1))\n",
    "                neg_distance = torch.sqrt(torch.sum((anchor_embeddings - negative_embeddings) ** 2, dim=1))\n",
    "            elif distance_metric == 'cosine':\n",
    "                # 对于cosine相似度，需要转换为距离\n",
    "                pos_distance = 1 - cos_sim(anchor_embeddings, positive_embeddings)\n",
    "                neg_distance = 1 - cos_sim(anchor_embeddings, negative_embeddings)\n",
    "            \n",
    "            positive_distances.extend(pos_distance.cpu().numpy().tolist())\n",
    "            negative_distances.extend(neg_distance.cpu().numpy().tolist())\n",
    "\n",
    "    return np.mean(positive_distances), np.mean(negative_distances)\n",
    "\n",
    "\n",
    "sts_dataset = load_dataset(\"stsb_multi_mt\", \"en\")\n",
    "# sts_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epoch 0, Loss: 1.3843977402468197, Spearman Correlation on STS: 0.8671631197908373， Positive Distance: 6.995271792475658e-08, Negative Distance: 0.1295417695167737\n"
     ]
    }
   ],
   "source": [
    "model = SentenceEmbeddingModel(\"sentence-transformers/all-MiniLM-L6-v2\", dropout_rate=0.1, noise_std=0.05).to('cuda')\n",
    "\n",
    "def collate_fn(batch):\n",
    "    anchor_sentences = [example[0] for example in batch]\n",
    "    positive_sentences = [example[1] for example in batch]\n",
    "    negative_sentences = [example[2] for example in batch]\n",
    "    return anchor_sentences, positive_sentences, negative_sentences\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16, collate_fn=collate_fn)\n",
    "margin = 1.0\n",
    "new_triplet_loss = CustomTripletLoss(margin=margin, distance_metric='euclidean', positive_margin=0.5)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "print(\"Start training\")\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        # print(batch)\n",
    "        # continue\n",
    "        anchor_sentences, positive_sentences, negative_sentences = batch\n",
    "        sentences = anchor_sentences  + negative_sentences\n",
    "        embeddings = model(sentences, apply_dropout=False, apply_noise=True)\n",
    "        positive_embeddings = model(positive_sentences, apply_dropout=True, apply_noise=True)\n",
    "     \n",
    "        anchor_embeddings = embeddings[:len(anchor_sentences)]\n",
    "        negative_embeddings = embeddings[len(anchor_sentences):]\n",
    "    \n",
    "        loss = new_triplet_loss(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    spearman_corr = evaluate_sts(model, sts_dataset)\n",
    "    pos_dist_avg, neg_dist_avg = calculate_distances(model, train_dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {train_loss/len(train_dataloader)}, Spearman Correlation on STS: {spearman_corr}， Positive Distance: {pos_dist_avg}, Negative Distance: {neg_dist_avg}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
